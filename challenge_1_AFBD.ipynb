{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "challenge-1_AFBD.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AuMNWouAx6u1",
        "ElocbeNttDK7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuMNWouAx6u1"
      },
      "source": [
        "# Env Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFH9zQGdta1S",
        "outputId": "35b1f10a-0ac7-40b3-ee95-413ede13621c"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark\n",
        "!pip install -q findspark\n",
        "!pip show pyspark\n",
        "!pip install memory_profiler\n",
        "%load_ext memory_profiler"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 29 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 14.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=74bd3b3a3da7074d74eb057d39749c7ee0ce80f0a9e886427c18adad2e4a3ad3\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
            "Name: pyspark\n",
            "Version: 3.2.0\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: py4j\n",
            "Required-by: \n",
            "Collecting memory_profiler\n",
            "  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory_profiler) (5.4.8)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=30190 sha256=3cc572e885a1b3094d4234d60fb8f40a87e4018a926a61da98195b0024880c38\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.58.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC4UOqUPthsv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4928d5c6-7391-43fa-83c2-94cccbe5b173"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/usr/local/lib/python3.7/dist-packages/pyspark\"\n",
        "\n",
        "import pyspark\n",
        "sc = pyspark.SparkContext(\"local[*]\").getOrCreate()\n",
        "sc"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4e198e564daf:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez3Ct5xrAvFV"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qo7uK-u6_Fs1"
      },
      "source": [
        "# Data set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ukt7gfZRyIFT",
        "outputId": "b14e16d5-91d2-4ae8-a58e-8fef4a6a27e9"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/aliswh/architectures-for-big-data/main/logDataset.csv\n",
        "log_path = '/content/logDataset.csv'\n",
        "data = sc.textFile(log_path)\n",
        "data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-04 12:12:02--  https://raw.githubusercontent.com/aliswh/architectures-for-big-data/main/logDataset.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118114 (115K) [text/plain]\n",
            "Saving to: ‘logDataset.csv’\n",
            "\n",
            "\rlogDataset.csv        0%[                    ]       0  --.-KB/s               \rlogDataset.csv      100%[===================>] 115.35K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-11-04 12:12:02 (76.9 MB/s) - ‘logDataset.csv’ saved [118114/118114]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/logDataset.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I37vCOuN_Aee",
        "outputId": "26378ac1-476d-4524-d68c-cce82155b7b5"
      },
      "source": [
        "data.count() # includes header"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "856"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsA2Zbah_CDb",
        "outputId": "22eae34c-27c6-44c5-deff-4039aed3cbfc"
      },
      "source": [
        "data.take(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CompID,CompName,CompRef,ErrorCode,ErrorText,ErrorType,FeederID,GoodPickUps,HammerGW.ts_load,InsertDate,McID,Milliseconds,NozzleNum,NozzleSize,Slot,Station,SubSlot,TimeStamp,Type',\n",
              " '076333,002930,0,No error,,1,10511,0,2021-10-29T06:56:02.905Z,2021-10-29T07:57:16.663Z,02,611,0,0,174,32,0,2021-10-29T07:57:16.610Z,null',\n",
              " '049198,075897,0,No error,,1,298522,12,2021-10-29T06:56:03.145Z,2021-10-29T07:57:18.710Z,10,513,0,0,194,88,0,2021-10-29T07:57:18.513Z,null',\n",
              " '041960,058189,0,No error,,1,86425640,0,2021-10-29T06:56:03.145Z,2021-10-29T07:57:45.443Z,06,433,0,0,27,24,0,2021-10-29T07:57:45.433Z,-1',\n",
              " '038137,031878,AC232,8000D701,,0,86425640,0,2021-10-29T06:56:03.145Z,2021-10-29T07:57:50.787Z,03,772,1,8,145,65,0,2021-10-29T07:57:50.773Z,-1',\n",
              " '031989,008144,AC231,8000D701,,0,86425640,0,2021-10-29T06:56:03.145Z,2021-10-29T07:57:50.850Z,09,782,1,5,191,36,0,2021-10-29T07:57:50.783Z,-1',\n",
              " '010575,075971,AC232,8000D701,,0,86425640,56,2021-10-29T06:56:03.145Z,2021-10-29T07:57:50.913Z,02,792,1,5,52,90,0,2021-10-29T07:57:50.793Z,-1',\n",
              " '022692,094799,0,No error,,1,86425640,0,2021-10-29T06:56:03.145Z,2021-10-29T07:57:50.757Z,09,922,0,0,108,65,0,2021-10-29T07:57:50.923Z,-1',\n",
              " '031965,047136,0,No error,,1,10511,20,2021-10-29T06:56:03.145Z,2021-10-29T07:57:52.523Z,10,581,0,0,156,23,0,2021-10-29T07:57:52.580Z,null',\n",
              " '054986,041592,C1419,8000D701,,0,86204927,62,2021-10-29T06:56:03.145Z,2021-10-29T07:58:45.320Z,02,291,1,8,35,19,0,2021-10-29T07:58:45.290Z,-1']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWhQQD8D_Phn"
      },
      "source": [
        "Write an algorithm to distribute the computation of mean and median over a dataset using Map/Reduce model.\n",
        "\n",
        "*I choose to compute the mean and the median over the 'Milliseconds' (12th) column.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "utIsmbih_P52",
        "outputId": "57e4aa6b-336e-4457-d240-3330daa894c6"
      },
      "source": [
        "# some preprocessing\n",
        "headers = data.first()  \n",
        "rdd =  data.filter(lambda line: line != headers)\n",
        "rdd = rdd.map(lambda line: line.split(\",\"))\n",
        "rdd.take(5)[0][11] # take first 'Milliseconds' value from first element in the file "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'611'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKPXCVgox_gL"
      },
      "source": [
        "# Naive Solution\n",
        "\n",
        "Especially the median is not scalable.\n",
        "* The median is a descriptive stat on data that is based on _position_\n",
        "* Our RDD is not ordered, so to get the central position (aka the median) we need to order it\n",
        "* Sorting has a cost...\n",
        "* ...but also collecting all the data in main memory to select the central value!\n",
        "Can we do better?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVWvjpgk_jAo",
        "outputId": "f4140f50-0f5c-4105-e31b-278bcef573ac"
      },
      "source": [
        "rdd.map(lambda x: int(x[11])).sum()/rdd.count() # mean"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "513.706432748538"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hZw2S7jS_jVw",
        "outputId": "92e6651b-ffac-4ef8-db95-bc7ed6ef4e78"
      },
      "source": [
        "rdd.sortBy(lambda x: int(x[11])).collect()[rdd.count()//2][11] # median"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'505'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLlyCoGmyEmz"
      },
      "source": [
        "# (Hopefully) better solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k87jklU-UO8v"
      },
      "source": [
        "## Computing the mean\n",
        "Strategy: use an **accumulator** for the count of all elements in the RDD.\n",
        "Computing the mean without an accumulator..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8x00mFCTF5Q"
      },
      "source": [
        "from operator import add\n",
        "pairs = rdd.map(lambda x: (int(x[11]), 1)).reduceByKey(add)\n",
        "count = pairs.map(lambda x: x[1]).sum()\n",
        "mean = pairs.map(lambda x: x[0]*x[1]).reduce(add)/count"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTKGVahoMVo7",
        "outputId": "903538ac-df8f-4892-d1be-97764ed63d11"
      },
      "source": [
        "print(f\"Example of pairs: {pairs.collect()[:5]}\\nresulting count of elements: {count}\\nresulting mean: {round(mean,1)}\") # check"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example of pairs: [(772, 1), (782, 2), (792, 3), (922, 2), (352, 1)]\n",
            "resulting count of elements: 855\n",
            "resulting mean: 513.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs68uGh1TTz8"
      },
      "source": [
        "...versus computing with an accumulator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFtytkmxS2KV",
        "outputId": "1b690d1a-4b55-49e9-9db3-463791b8c68d"
      },
      "source": [
        "count_accumulator = sc.accumulator(pairs.map(lambda x: x[1]).sum())\n",
        "print(f\"Accumulator value: {count_accumulator}\")\n",
        "mean = pairs.map(lambda x: x[0]*x[1]).reduce(add)/count_accumulator.value\n",
        "print(f\"Mean: {round(mean,1)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accumulator value: 855\n",
            "Mean: 513.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g02XX22NUJvP"
      },
      "source": [
        "## Computing the median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axTya_FGXnLz"
      },
      "source": [
        "We don't want to collect a huge array in main memory only to get one value, instead we can\n",
        "* use TakeOrdered, which is an Action that doesn't need to the sortByKey transformation, however it doesn't work if we have duplicate values\n",
        "* order the RDD, **index** its elements, filter based on the condition that the index is equal to the rdd.count(), which we stored in a **broadcast variable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msR66zexXeYY",
        "outputId": "635d68dd-2fe4-45d2-d3ff-2dcd3160db0f"
      },
      "source": [
        "rdd.map(lambda x: int(x[11])).takeOrdered(count_accumulator.value//2)[-1] # not the real median, because 'take' and 'takeOrderd' don't consider duplicate values"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "503"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np9otRmSek3b",
        "outputId": "a80b62a8-0661-4a82-e21e-9c9fc4f9fdea"
      },
      "source": [
        "count_broadcast = sc.broadcast(count_accumulator.value)\n",
        "def is_broadcast(x):\n",
        "  return int(x[1]) == count_broadcast.value//2\n",
        "  \n",
        "rdd.map(lambda x: (int(x[11]), 1)).sortByKey().zipWithIndex().filter(is_broadcast).collect()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((505, 1), 427)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YarruUkAjwt2"
      },
      "source": [
        "# Conclusions\n",
        "What can be done better?\n",
        "* The final solution shares the `pairs` RDD\n",
        "* I don't think there could be improvements for the `mean`, there aren't many different ways of computing sums and multiplication that I can think of, and the accumulator is the best practice that I ended up implementing\n",
        "* A lot can be said about the `median`: I don't think this is the final best solution, also because the dataset provided is so small that it is diffucult to measure the efficiency of the \"smartest\" solution. \n",
        "  * We still need to sort the RDD, which comes with all the problems of the sorting process\n",
        "  * The zipWithIndex() function could be problematic, maybe there are better ways of indexing a sorted RDD, or not doing it at all\n",
        "  * The filter is applied over all the RDD, while we wish that it stops checking for a median when it is found.\n",
        "  * Still, this solution doesn't require to store all the sorted array in main memory, because we collect only what is not filter by the condition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElocbeNttDK7"
      },
      "source": [
        "## Time and RAM consumed by both processes (including printing time)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njF7ImKfigKv",
        "outputId": "658202c4-4216-43f0-9015-9197d978a17e"
      },
      "source": [
        "%%memit\n",
        "count = rdd.count()\n",
        "print(rdd.map(lambda x: int(x[11])).sum()/count) # mean\n",
        "print(rdd.sortBy(lambda x: x[11]).collect()[count//2][11]) # median"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513.706432748538\n",
            "559\n",
            "peak memory: 133.05 MiB, increment: 0.61 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4JtN0_6ik--",
        "outputId": "e962c5bb-a7ad-40a1-d343-004f37ae2402"
      },
      "source": [
        "%%time\n",
        "count = rdd.count()\n",
        "print(rdd.map(lambda x: int(x[11])).sum()/count) # mean\n",
        "print(rdd.sortBy(lambda x: x[11]).collect()[count//2][11]) # median"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513.706432748538\n",
            "559\n",
            "CPU times: user 77.4 ms, sys: 9.8 ms, total: 87.2 ms\n",
            "Wall time: 790 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcxx0vuDjPCZ",
        "outputId": "29c8132d-10a1-421e-b170-9ae15cf12269"
      },
      "source": [
        "%%memit\n",
        "pairs = rdd.map(lambda x: (int(x[11]), 1))\n",
        "count_accumulator = sc.accumulator(pairs.count())\n",
        "count_broadcast = sc.broadcast(count_accumulator.value)\n",
        "\n",
        "def is_broadcast(x):\n",
        "  return int(x[1]) == count_broadcast.value//2\n",
        "\n",
        "print(pairs.reduceByKey(add).map(lambda x: x[0]*x[1]).reduce(add)/count_accumulator.value) # mean\n",
        "print(pairs.sortByKey().zipWithIndex().filter(is_broadcast).collect()[0][0][0]) # median"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513.706432748538\n",
            "505\n",
            "peak memory: 133.28 MiB, increment: 0.03 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rhU98g4iUBp",
        "outputId": "7033d6f2-4af0-4621-85e1-4be02d422862"
      },
      "source": [
        "%%time\n",
        "pairs = rdd.map(lambda x: (int(x[11]), 1))\n",
        "count_accumulator = sc.accumulator(pairs.count())\n",
        "count_broadcast = sc.broadcast(count_accumulator.value)\n",
        "\n",
        "def is_broadcast(x):\n",
        "  return int(x[1]) == count_broadcast.value//2\n",
        "\n",
        "print(pairs.reduceByKey(add).map(lambda x: x[0]*x[1]).reduce(add)/count_accumulator.value) # mean\n",
        "print(pairs.sortByKey().zipWithIndex().filter(is_broadcast).collect()[0][0][0]) # median"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "513.706432748538\n",
            "505\n",
            "CPU times: user 101 ms, sys: 12.1 ms, total: 114 ms\n",
            "Wall time: 1.04 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoAb7MAdvvsS"
      },
      "source": [
        "## Docker\n",
        "Note: This notebook was successfully run on Docker using the [Jupyter PySpark Notebook](https://hub.docker.com/r/jupyter/pyspark-notebook), by running \n",
        "\n",
        "`Docker run -p 8888:8888 jupyter/pyspark-notebook` \n",
        "\n",
        "to download the image, and \n",
        "\n",
        "`docker cp C:\\Users\\alice\\Downloads\\logDataset.csv <container-id>:/log.csv` \n",
        "\n",
        "to import the .csv file.\n"
      ]
    }
  ]
}